{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13671710,"sourceType":"datasetVersion","datasetId":8693075},{"sourceId":13729513,"sourceType":"datasetVersion","datasetId":8735110},{"sourceId":13770133,"sourceType":"datasetVersion","datasetId":8763914},{"sourceId":646071,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":487233,"modelId":502656},{"sourceId":646692,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":487720,"modelId":503141}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes peft datasets trl sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:06:12.646601Z","iopub.execute_input":"2025-11-18T04:06:12.646816Z","iopub.status.idle":"2025-11-18T04:07:45.016575Z","shell.execute_reply.started":"2025-11-18T04:06:12.646795Z","shell.execute_reply":"2025-11-18T04:07:45.015654Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:07:47.414915Z","iopub.execute_input":"2025-11-18T04:07:47.415221Z","iopub.status.idle":"2025-11-18T04:08:07.348155Z","shell.execute_reply.started":"2025-11-18T04:07:47.415176Z","shell.execute_reply":"2025-11-18T04:08:07.347259Z"}},"outputs":[{"name":"stdout","text":"Collecting chromadb\n  Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.12.4)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.15.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.37.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.37.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.22.1)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.3)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.0)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.2.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nCollecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (6.33.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nCollecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\nDownloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.6/456.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=beaee73d98dd3c1a58bc88aa90721125633dfaa96b5144370edd4af004eb2829\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, cachetools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, chromadb\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 6.2.1\n    Uninstalling cachetools-6.2.1:\n      Successfully uninstalled cachetools-6.2.1\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.37.0\n    Uninstalling opentelemetry-api-1.37.0:\n      Successfully uninstalled opentelemetry-api-1.37.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.37.0\n    Uninstalling opentelemetry-sdk-1.37.0:\n      Successfully uninstalled opentelemetry-sdk-1.37.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 cachetools-5.5.2 chromadb-1.3.4 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport traceback\nfrom typing import Dict, List, Tuple\nimport numpy as np\nimport torch\n\n# LLM / embeddings libs (ensure installed)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer\n\n# chromadb\nimport chromadb\nfrom chromadb.config import Settings\n\n# -----------------------\n# Initialize models (LoRA + Mistral)\n# -----------------------\nfrom peft import PeftModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nMODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nLORA_DIR   = \"/kaggle/input/generator/other/default/1/generator_lora_25k\"\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nprint(f\"Loading tokenizer + base model + LoRA from {LORA_DIR} ...\")\n\nprint(f\"Loading tokenizer + base model + LoRA from {LORA_DIR} ...\")\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\",\n    )\n\n    # Attach LoRA\n    model = PeftModel.from_pretrained(\n        base_model,\n        LORA_DIR,\n        use_bfloat16=False,\n        use_bnb=False\n    )\n\n    # ğŸ”¥ FORCE GPU USAGE\n    model.to(device)\n\n    model.eval()\n    print(\"âœ“ LoRA model loaded successfully on GPU!\")\n\nexcept Exception as e:\n    print(f\"âŒ Failed to load LoRA model: {e}\")\n    tokenizer = None\n    model = None\n\n\n# -----------------------\n# Load Embeddings Model\n# -----------------------\nprint(f\"Loading embedding model: {EMBEDDING_MODEL_NAME} ...\")\ntry:\n    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\nexcept Exception as e:\n    print(f\"Warning: failed to load embedding model {EMBEDDING_MODEL_NAME}: {e}\")\n    embedding_model = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:51:17.835136Z","iopub.execute_input":"2025-11-17T22:51:17.835438Z","iopub.status.idle":"2025-11-17T22:52:44.659562Z","shell.execute_reply.started":"2025-11-17T22:51:17.835395Z","shell.execute_reply":"2025-11-17T22:52:44.658786Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:51:22.997276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763419883.019492     176 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763419883.026298     176 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading tokenizer + base model + LoRA from /kaggle/input/generator/other/default/1/generator_lora_25k ...\nLoading tokenizer + base model + LoRA from /kaggle/input/generator/other/default/1/generator_lora_25k ...\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f84413bcc6468bbc779abd4fb617c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b7b2cbafbc43099a7546f8ad95e976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dd71b1edcfb45b6a6331d0c66f24ff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eedb2ec956ea429a9f25659c0b538125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0c1866cd1a4f8ab531780cc3090e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45e77a943784de888e5609623009082"}},"metadata":{}},{"name":"stdout","text":"âœ“ LoRA model loaded successfully on GPU!\nLoading embedding model: sentence-transformers/all-MiniLM-L6-v2 ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f674512ad274903969721fe65a6a543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ad44f747c3442788d8694d77229804"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab7780531014387b0183991cc0d10be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a02b61e62a4fc993bfc3c3a27d7931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5ba8614fb94b2fbbc39412592fe857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6e973810a04cabb2278f9e09ab1dd7"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# -----------------------\n# Chroma vector DB (persisted)\n# -----------------------\nfrom chromadb import PersistentClient\n\nchroma_client = PersistentClient(path=\"./.chroma\")\ncollection_name = \"rag_chunks\"\n# Create collection if not exists, else get it\ntry:\n    if collection_name in [c.name for c in chroma_client.list_collections()]:\n        collection = chroma_client.get_collection(name=collection_name)\n    else:\n        collection = chroma_client.create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\nexcept Exception as e:\n    # fallback: create\n    print(f\"Warning creating/getting Chroma collection: {e}\")\n    collection = chroma_client.create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n\n# -----------------------\n# Helper functions\n# -----------------------\ndef generate_text(prompt: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:\n    \"\"\"Generate text using the loaded model/tokenizer. If model/tokenizer missing, returns a placeholder.\"\"\"\n    if model is None or tokenizer is None:\n        return \"MODEL_NOT_LOADED\"\n    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            top_p=0.95,\n            top_k=50,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if \"[/INST]\" in generated_text:\n        generated_text = generated_text.split(\"[/INST]\")[-1].strip()\n    return generated_text\n\ndef generate_embeddings_batch(chunks_batch: List[str]) -> List[List[float]]:\n    \"\"\"Return list of embeddings (lists). If embedding_model missing, return zeros.\"\"\"\n    if embedding_model is None:\n        # fallback: zeros\n        dim = 384\n        return [[0.0] * dim for _ in chunks_batch]\n    embeddings = embedding_model.encode(chunks_batch, convert_to_numpy=True)\n    # embeddings shape (n, dim)\n    return embeddings.tolist()\n\n# -----------------------\n# Data loading / preprocessing\n# -----------------------\ndef load_glaive_dataset(file_path: str, num_samples: int = 1000) -> List[Dict[str, str]]:\n    print(f\"Loading Glaive dataset from {file_path}...\")\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    random.shuffle(data)\n    data = data[:num_samples]\n    print(f\"Loaded {len(data)} samples from Glaive dataset\")\n    return data\n\ndef create_document_chunks_from_glaive(data: List[Dict[str, str]]) -> List[str]:\n    chunks = []\n    for idx, item in enumerate(data, 1):\n        question = item.get('question', '')\n        answer = item.get('answer', '')\n        chunk = f\"<id:{idx}>\\n{question}\\n{answer}\\n</id:{idx}>\"\n        chunks.append(chunk)\n    print(f\"Created {len(chunks)} document chunks for RAG retrieval\")\n    return chunks\n\ndef preprocess_text(text: str) -> str:\n    return text.lower()\n\ndef preprocess_chunks(chunks: List[str]) -> List[str]:\n    return [preprocess_text(chunk) for chunk in chunks]\n\n# -----------------------\n# Embeddings + vector store\n# -----------------------\ndef generate_embeddings(chunks: List[str], batch_size: int = 32, verbose: bool = False) -> np.ndarray:\n    all_embeddings = []\n    if verbose:\n        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n    for i in range(0, len(chunks), batch_size):\n        batch = chunks[i:i + batch_size]\n        embeddings = generate_embeddings_batch(batch)\n        all_embeddings.extend(embeddings)\n        if verbose and ((i + batch_size) % 100 == 0 or i + batch_size >= len(chunks)):\n            print(f\"Processed {min(i + batch_size, len(chunks))}/{len(chunks)} chunks\")\n    return np.array(all_embeddings)\n\ndef save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n    try:\n        with open(output_file, 'w', encoding='utf-8') as file:\n            json.dump(embeddings.tolist(), file)\n        print(f\"Saved embeddings to {output_file}\")\n    except Exception as e:\n        print(f\"Warning: failed to save embeddings to {output_file}: {e}\")\n\ndef add_to_vector_store(embeddings: np.ndarray, chunks: List[str]):\n    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n    try:\n        collection.add(\n            ids=ids,\n            embeddings=embeddings.tolist(),\n            documents=chunks\n        )\n        print(f\"Added {len(chunks)} chunks to Chroma collection '{collection_name}'\")\n    except Exception as e:\n        print(f\"Error adding to vector store: {e}\")\n        traceback.print_exc()\n\ndef similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n    try:\n        results = collection.query(\n            query_embeddings=[query_embedding.tolist()],\n            n_results=top_k\n        )\n        # results is a dict; documents is a list of lists\n        return results.get(\"documents\", [[]])[0]\n    except Exception as e:\n        print(f\"Warning during similarity_search: {e}\")\n        return []\n\ndef retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n    query_embedding = generate_embeddings([query_text])[0]\n    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n    return relevant_chunks\n\n# -----------------------\n# Reward functions\n# -----------------------\ndef calculate_rouge_score(response: str, ground_truth: str) -> float:\n    def lcs_length(s1, s2):\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i-1] == s2[j-1]:\n                    dp[i][j] = dp[i-1][j-1] + 1\n                else:\n                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n        return dp[m][n]\n    resp_tokens = response.lower().split()\n    gt_tokens = ground_truth.lower().split()\n    if len(resp_tokens) == 0 or len(gt_tokens) == 0:\n        return 0.0\n    lcs_len = lcs_length(resp_tokens, gt_tokens)\n    precision = lcs_len / len(resp_tokens) if len(resp_tokens) > 0 else 0\n    recall = lcs_len / len(gt_tokens) if len(gt_tokens) > 0 else 0\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndef calculate_token_overlap(response: str, ground_truth: str) -> float:\n    resp_tokens = set(response.lower().split())\n    gt_tokens = set(ground_truth.lower().split())\n    if len(gt_tokens) == 0:\n        return 0.0\n    intersection = resp_tokens & gt_tokens\n    return len(intersection) / len(gt_tokens)\n\ndef detect_hallucination(response: str, context_chunks: List[str]) -> float:\n    response_lower = response.lower()\n    context_combined = \" \".join(context_chunks).lower()\n    resp_terms = [w for w in response_lower.split() if len(w) > 3]\n    if len(resp_terms) == 0:\n        return 0.0\n    found_terms = sum(1 for term in resp_terms if term in context_combined)\n    coverage = found_terms / len(resp_terms)\n    return coverage\n\ndef calculate_context_relevance(query: str, context_chunks: List[str]) -> float:\n    if not context_chunks:\n        return 0.0\n    query_embedding = generate_embeddings([query])[0]\n    context_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n    similarities = [np.dot(query_embedding, ctx_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(ctx_emb) + 1e-12) for ctx_emb in context_embeddings]\n    return float(np.mean(similarities)) if similarities else 0.0\n\ndef cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    if norm_vec1 == 0 or norm_vec2 == 0:\n        return 0.0\n    return float(dot_product / (norm_vec1 * norm_vec2))\n\ndef calculate_hybrid_reward(\n    response: str,\n    ground_truth: str,\n    query: str,\n    context_chunks: List[str],\n    weights: Dict[str, float] = None\n) -> Dict[str, float]:\n    if weights is None:\n        weights = {\n            'semantic_similarity': 0.35,\n            'rouge_score': 0.25,\n            'token_overlap': 0.15,\n            'hallucination_penalty': 0.15,\n            'context_relevance': 0.10\n        }\n    response_embedding = generate_embeddings([response])[0]\n    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n    semantic_sim = cosine_similarity(response_embedding, ground_truth_embedding)\n    rouge_score = calculate_rouge_score(response, ground_truth)\n    token_overlap = calculate_token_overlap(response, ground_truth)\n    hallucination_score = detect_hallucination(response, context_chunks)\n    context_relevance = calculate_context_relevance(query, context_chunks)\n    hybrid_reward = (\n        weights['semantic_similarity'] * semantic_sim +\n        weights['rouge_score'] * rouge_score +\n        weights['token_overlap'] * token_overlap +\n        weights['hallucination_penalty'] * hallucination_score +\n        weights['context_relevance'] * context_relevance\n    )\n    return {\n        'total_reward': hybrid_reward,\n        'semantic_similarity': semantic_sim,\n        'rouge_score': rouge_score,\n        'token_overlap': token_overlap,\n        'hallucination_score': hallucination_score,\n        'context_relevance': context_relevance\n    }\n\n# -----------------------\n# LLM response helpers\n# -----------------------\ndef construct_prompt(query: str, context_chunks: List[str]) -> str:\n    context = \"\\n\".join(context_chunks)\n    prompt = f\"\"\"You are a helpful coding assistant. Use the provided context to answer the question. If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n    return prompt\n\ndef generate_response(prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:\n    return generate_text(prompt, max_new_tokens=max_tokens, temperature=temperature)\n\ndef basic_rag_pipeline(query: str) -> str:\n    relevant_chunks = retrieve_relevant_chunks(query)\n    prompt = construct_prompt(query, relevant_chunks)\n    response = generate_response(prompt)\n    return response\n\n# -----------------------\n# RL components: state/actions\n# -----------------------\ndef define_state(\n    query: str,\n    context_chunks: List[str],\n    rewritten_query: str = None,\n    previous_responses: List[str] = None,\n    previous_rewards: List[float] = None\n) -> dict:\n    return {\n        \"original_query\": query,\n        \"current_query\": rewritten_query if rewritten_query else query,\n        \"context\": context_chunks,\n        \"previous_responses\": previous_responses if previous_responses else [],\n        \"previous_rewards\": previous_rewards if previous_rewards else []\n    }\n\ndef define_action_space() -> List[str]:\n    return [\"rewrite_query\", \"expand_context\", \"filter_context\", \"generate_response\"]\n\n# -----------------------\n# Load / initialize Q_VALUES (persisted)\n# -----------------------\nQ_FILE = \"q_values.json\"\ntry:\n    if os.path.exists(Q_FILE):\n        with open(Q_FILE, \"r\") as f:\n            loaded = json.load(f)\n        Q_VALUES = {a: float(loaded.get(a, 0.0)) for a in define_action_space()}\n        print(f\"Loaded saved Q-values from {Q_FILE}\")\n    else:\n        Q_VALUES = {a: 0.0 for a in define_action_space()}\nexcept Exception as e:\n    print(f\"Warning: could not load {Q_FILE}: {e}\")\n    Q_VALUES = {a: 0.0 for a in define_action_space()}\n\n# -----------------------\n# Query rewrite / context ops\n# -----------------------\ndef rewrite_query(query: str, context_chunks: List[str], max_tokens: int = 100, temperature: float = 0.3) -> str:\n    rewrite_prompt = f\"\"\"You are a query optimization assistant. Rewrite the given query to make it more effective for retrieving relevant information.\n\nOriginal query: {query}\n\nContext so far:\n{' '.join(context_chunks[:2]) if context_chunks else 'No context available yet'}\n\nProvide ONLY the rewritten query, nothing else. Make it more specific and targeted for better document retrieval.\n\nRewritten query:\"\"\"\n    rewritten_query = generate_text(rewrite_prompt, max_new_tokens=max_tokens, temperature=temperature)\n    return rewritten_query.strip()\n\ndef expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n    new_chunks = [chunk for chunk in additional_chunks if chunk not in current_chunks]\n    expanded_context = current_chunks + new_chunks[:top_k]\n    return expanded_context\n\ndef filter_context(query: str, context_chunks: List[str]) -> List[str]:\n    if not context_chunks:\n        return []\n    query_embedding = generate_embeddings([query])[0]\n    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n    relevance_scores = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunk_embeddings]\n    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n    return filtered_chunks\n\n# -----------------------\n# Q-learning policy\n# -----------------------\ndef policy_network_q(epsilon: float = 0.1, train_mode: bool = True) -> str:\n    action_space = define_action_space()\n    if train_mode and np.random.random() < epsilon:\n        return random.choice(action_space)\n    return max(Q_VALUES, key=Q_VALUES.get)\n\ndef update_q_value(action: str, reward: float, lr: float = 0.1):\n    old_value = Q_VALUES.get(action, 0.0)\n    Q_VALUES[action] = old_value + lr * (reward - old_value)\n\n# -----------------------\n# RL step using Q-learning selection\n# -----------------------\ndef rl_step(state: dict, ground_truth: str, query: str, train_mode: bool = True) -> tuple:\n    action = policy_network_q(epsilon=0.1, train_mode=train_mode)\n    response = None\n    reward_info = None\n\n    # ---------- INTERMEDIATE ACTIONS ----------\n    if action == \"rewrite_query\":\n        old_query = state[\"current_query\"]\n        rewritten_query = rewrite_query(old_query, state[\"context\"])\n        if rewritten_query:\n            state[\"current_query\"] = rewritten_query\n            state[\"context\"] = retrieve_relevant_chunks(rewritten_query)\n\n        # Small reward: Did rewriting improve context?\n        reward = calculate_context_relevance(query, state[\"context\"])\n        if train_mode:\n            update_q_value(action, reward)\n        reward_info = {\"total_reward\": reward}\n\n    elif action == \"expand_context\":\n        old_len = len(state[\"context\"])\n        state[\"context\"] = expand_context(state[\"current_query\"], state[\"context\"])\n\n        # Small reward: Did context grow?\n        reward = len(state[\"context\"]) - old_len\n        reward = max(0, reward * 0.05)  # tiny reward\n        if train_mode:\n            update_q_value(action, reward)\n        reward_info = {\"total_reward\": reward}\n\n    elif action == \"filter_context\":\n        old_context = state[\"context\"]\n        state[\"context\"] = filter_context(state[\"current_query\"], state[\"context\"])\n\n        # Small reward: Did it become more relevant?\n        reward = calculate_context_relevance(query, state[\"context\"])\n        if train_mode:\n            update_q_value(action, reward)\n        reward_info = {\"total_reward\": reward}\n\n    # ---------- FINAL ACTION ----------\n    elif action == \"generate_response\":\n        prompt = construct_prompt(state[\"current_query\"], state[\"context\"])\n        response = generate_response(prompt)\n        reward_info = calculate_hybrid_reward(response, ground_truth, query, state[\"context\"])\n        state[\"previous_responses\"].append(response)\n        state[\"previous_rewards\"].append(reward_info['total_reward'])\n\n        if train_mode:\n            update_q_value(action, reward_info['total_reward'])\n\n    return state, action, reward_info, response\n\n\n# -----------------------\n# Training loop\n# -----------------------\ndef training_loop(\n    training_samples: List[Dict[str, str]],\n    num_episodes: int = 100,\n    samples_per_episode: int = 100\n) -> Tuple[List[float], List[Dict], List[str]]:\n    all_rewards = []\n    all_metrics = []\n    best_responses = []\n\n    print(f\"\\nStarting RL training for {num_episodes} episodes...\")\n    print(f\"Using {samples_per_episode} random samples per episode\\n\")\n\n    for episode in range(num_episodes):\n        episode_samples = random.sample(training_samples, min(samples_per_episode, len(training_samples)))\n        episode_rewards = []\n        episode_metrics = []\n\n        for sample_idx, sample in enumerate(episode_samples):\n            # progress line\n            print(f\"Episode {episode+1}/{num_episodes} | Progress: {sample_idx+1}/{len(episode_samples)} samples\", end=\"\\r\", flush=True)\n\n            query = sample['question']\n            ground_truth = sample['answer']\n            context_chunks = retrieve_relevant_chunks(query)\n            state = define_state(query, context_chunks)\n\n            reward_info = None\n            response = None\n            for step in range(5):\n                state, action, reward_info, response = rl_step(state, ground_truth, query, train_mode=True)\n                if response and reward_info:\n                    episode_rewards.append(reward_info['total_reward'])\n                    episode_metrics.append(reward_info)\n                    break\n\n            # if RL didn't produce a response in the steps, force a generate_response\n            if reward_info is None:\n                prompt = construct_prompt(state[\"current_query\"], state[\"context\"])\n                response = generate_response(prompt)\n                reward_info = calculate_hybrid_reward(response, ground_truth, query, state[\"context\"])\n                episode_rewards.append(reward_info['total_reward'])\n                episode_metrics.append(reward_info)\n                update_q_value(\"generate_response\", reward_info['total_reward'])\n\n        avg_episode_reward = np.mean(episode_rewards) if episode_rewards else 0.0\n        all_rewards.append(avg_episode_reward)\n\n        print(f\"\\nEpisode {episode+1}/{num_episodes}: Avg Reward = {avg_episode_reward:.4f}\")\n\n    # --- persist average rewards and Q-values for plotting / re-use ---\n    try:\n        with open(\"training_avg_rewards.json\", \"w\") as f:\n            json.dump(all_rewards, f)\n        print(\"Saved per-episode average rewards to training_avg_rewards.json\")\n    except Exception as e:\n        print(f\"Warning: could not save training_avg_rewards.json: {e}\")\n\n    try:\n        with open(\"q_values.json\", \"w\") as f:\n            json.dump(Q_VALUES, f)\n        print(\"Saved Q-values to q_values.json\")\n    except Exception as e:\n        print(f\"Warning: could not save q_values.json: {e}\")\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Training completed!\")\n    print(f\"Final Average Reward: {np.mean(all_rewards[-10:]) if all_rewards else 0.0:.4f}\")\n    print(f\"{'='*60}\")\n    return all_rewards, all_metrics, best_responses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:52:52.812419Z","iopub.execute_input":"2025-11-17T22:52:52.812706Z","iopub.status.idle":"2025-11-17T22:52:52.861593Z","shell.execute_reply.started":"2025-11-17T22:52:52.812684Z","shell.execute_reply":"2025-11-17T22:52:52.860872Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# -----------------------\n# Comparison / evaluation\n# -----------------------\ndef compare_rag_on_multiple_samples(\n    test_samples: List[Dict[str, str]],\n    training_samples: List[Dict[str, str]],\n    num_test_samples: int = 10\n) -> Dict:\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"COMPARING RAG APPROACHES ON {num_test_samples} TEST SAMPLES\")\n    print(\"=\" * 80)\n\n    print(\"\\n[1/2] Training RL-enhanced RAG...\")\n    rl_rewards, rl_metrics, _ = training_loop(training_samples, num_episodes=5, samples_per_episode=100)\n\n    print(f\"\\n[2/2] Testing on {num_test_samples} samples...\")\n    simple_rag_results = []\n    rl_rag_results = []\n\n    for idx, sample in enumerate(test_samples[:num_test_samples]):\n        query = sample['question']\n        ground_truth = sample['answer']\n\n        print(f\"\\n--- Test Sample {idx + 1}/{num_test_samples} ---\")\n        print(f\"Query: {query[:100]}...\")\n\n        # Simple RAG baseline\n        simple_response = basic_rag_pipeline(query)\n        simple_context = retrieve_relevant_chunks(query)\n        simple_metrics = calculate_hybrid_reward(simple_response, ground_truth, query, simple_context)\n        simple_rag_results.append(simple_metrics)\n\n        # RL-enhanced RAG (greedy policy during test)\n        context_chunks = retrieve_relevant_chunks(query)\n        state = define_state(query, context_chunks)\n\n        reward_info = None\n        response = None\n        for step in range(5):\n            state, action, reward_info, response = rl_step(state, ground_truth, query, train_mode=False)\n            if response and reward_info:\n                rl_rag_results.append(reward_info)\n                break\n\n        if reward_info is None:\n            prompt = construct_prompt(state[\"current_query\"], state[\"context\"])\n            response = generate_response(prompt)\n            reward_info = calculate_hybrid_reward(response, ground_truth, query, state[\"context\"])\n            rl_rag_results.append(reward_info)\n\n        rl_reward = reward_info['total_reward'] if reward_info else 0.0\n        print(f\"Simple RAG Reward: {simple_metrics['total_reward']:.4f}\")\n        print(f\"RL-RAG Reward: {rl_reward:.4f}\")\n\n    simple_avg = np.mean([r['total_reward'] for r in simple_rag_results]) if simple_rag_results else 0.0\n    rl_avg = np.mean([r['total_reward'] for r in rl_rag_results]) if rl_rag_results else 0.0\n    improvement = ((rl_avg - simple_avg) / simple_avg * 100) if simple_avg > 0 else 0\n\n    results = {\n        'simple_rag': {'avg_reward': float(simple_avg), 'detailed_results': simple_rag_results},\n        'rl_rag': {'avg_reward': float(rl_avg), 'detailed_results': rl_rag_results, 'training_rewards': rl_rewards},\n        'improvement_percentage': float(improvement),\n        'num_test_samples': num_test_samples\n    }\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL EVALUATION RESULTS\")\n    print(\"=\" * 80)\n    print(f\"Simple RAG Average Reward:      {simple_avg:.4f}\")\n    print(f\"RL-enhanced RAG Average Reward: {rl_avg:.4f}\")\n    print(f\"Improvement:                    {improvement:.2f}%\")\n    print(\"=\" * 80)\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:52:58.117565Z","iopub.execute_input":"2025-11-17T22:52:58.117869Z","iopub.status.idle":"2025-11-17T22:52:58.128205Z","shell.execute_reply.started":"2025-11-17T22:52:58.117845Z","shell.execute_reply":"2025-11-17T22:52:58.127434Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -----------------------\n# Main execution\n# -----------------------\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RAG with RL using Glaive Code Assistant Dataset\")\n    print(\"=\" * 80)\n    print(f\"\\nModel: {MODEL_NAME}\")\n    print(f\"Embedding Model: {EMBEDDING_MODEL_NAME}\")\n    print(f\"Device: {device}\")\n    print(\"=\" * 80 + \"\\n\")\n\n    glaive_file_path = \"/kaggle/input/glaive-coding-assistant/c9bc9129-eba0-4b10-8292-4ae70fc7fa0d.json\"\n\n    try:\n        glaive_data = load_glaive_dataset(glaive_file_path, num_samples=1000)\n        training_data = glaive_data[:900]\n        test_data = glaive_data[900:]\n        document_chunks = create_document_chunks_from_glaive(glaive_data)\n        preprocessed_chunks = preprocess_chunks(document_chunks)\n\n        print(f\"ğŸ“š Loaded {len(glaive_data)} total samples\")\n        print(f\"ğŸ‹ï¸ Training samples: {len(training_data)}\")\n        print(f\"ğŸ§ª Test samples: {len(test_data)}\")\n        print(f\"ğŸ“„ Document chunks for RAG: {len(document_chunks)}\\n\")\n\n        # save the loaded 1000 Q/A to a text file for later inspection\n        try:\n            with open(\"glaive_1000_qas.txt\", \"w\", encoding=\"utf-8\") as out:\n                for i, item in enumerate(glaive_data, 1):\n                    q = item.get(\"question\", \"\").strip()\n                    a = item.get(\"answer\", \"\").strip()\n                    out.write(f\"### Sample {i}\\nQuestion:\\n{q}\\n\\nAnswer:\\n{a}\\n\\n\\n\")\n            print(\"âœ“ Saved 1000 Q/A to glaive_1000_qas.txt\")\n        except Exception as e:\n            print(f\"Warning: could not save glaive_1000_qas.txt: {e}\")\n\n        print(\"Generating embeddings for document chunks...\")\n        embeddings = generate_embeddings(preprocessed_chunks, verbose=True)\n        save_embeddings(embeddings, \"embeddings.json\")\n        add_to_vector_store(embeddings, preprocessed_chunks)\n\n        \n\n        print(\"âœ“ Vector store ready!\\n\")\n\n        results = compare_rag_on_multiple_samples(test_samples=test_data, training_samples=training_data, num_test_samples=10)\n\n        # save results\n        try:\n            with open('rl_rag_multi_sample_results.json', 'w') as f:\n                json.dump(results, f, indent=2, default=str)\n            print(\"\\nâœ“ Results saved to 'rl_rag_multi_sample_results.json'\")\n        except Exception as e:\n            print(f\"Warning: could not save results file: {e}\")\n\n    except FileNotFoundError:\n        print(f\"âŒ Error: Could not find Glaive dataset at {glaive_file_path}\")\n    except Exception as e:\n        print(f\"âŒ Error occurred: {str(e)}\")\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:53:00.369315Z","iopub.execute_input":"2025-11-17T22:53:00.369630Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRAG with RL using Glaive Code Assistant Dataset\n================================================================================\n\nModel: mistralai/Mistral-7B-v0.1\nEmbedding Model: sentence-transformers/all-MiniLM-L6-v2\nDevice: cuda\n================================================================================\n\nLoading Glaive dataset from /kaggle/input/glaive-coding-assistant/c9bc9129-eba0-4b10-8292-4ae70fc7fa0d.json...\nLoaded 1000 samples from Glaive dataset\nCreated 1000 document chunks for RAG retrieval\nğŸ“š Loaded 1000 total samples\nğŸ‹ï¸ Training samples: 900\nğŸ§ª Test samples: 100\nğŸ“„ Document chunks for RAG: 1000\n\nâœ“ Saved 1000 Q/A to glaive_1000_qas.txt\nGenerating embeddings for document chunks...\nGenerating embeddings for 1000 chunks...\nProcessed 800/1000 chunks\nProcessed 1000/1000 chunks\nSaved embeddings to embeddings.json\nAdded 1000 chunks to Chroma collection 'rag_chunks'\nâœ“ Vector store ready!\n\n\n================================================================================\nCOMPARING RAG APPROACHES ON 10 TEST SAMPLES\n================================================================================\n\n[1/2] Training RL-enhanced RAG...\n\nStarting RL training for 5 episodes...\nUsing 100 random samples per episode\n\nEpisode 1/5 | Progress: 100/100 samples\nEpisode 1/5: Avg Reward = 0.2696\nEpisode 2/5 | Progress: 5/100 samples\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Load rewards from JSON\njson_path = \"/kaggle/working/training_avg_rewards.json\"\nwith open(json_path, \"r\") as f:\n    avg_rewards = json.load(f)\n\n# Create x-axis values (episodes)\nepisodes = list(range(1, len(avg_rewards) + 1))\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.plot(episodes, avg_rewards, marker='o', linewidth=2)\nplt.title(\"Average Reward per Episode\", fontsize=20)\nplt.xlabel(\"Episode\", fontsize=14)\nplt.ylabel(\"Average Reward\", fontsize=14)\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.xticks(episodes)  # Show all episode numbers\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# -------------------------------------------------------------------------\n# LOAD JSON (from file OR fallback to provided JSON content)\n# -------------------------------------------------------------------------\n\njson_path = Path(\"/kaggle/working/rl_rag_multi_sample_results.json\")\n\nif json_path.exists():\n    data = json.loads(json_path.read_text())\nelse:\n    # fallback â€“ paste the JSON content you gave earlier\n    data = json.loads(r'''\n    PASTE YOUR FULL JSON HERE IF NEEDED\n    ''')\n\n# -------------------------------------------------------------------------\n# EXTRACT DATA\n# -------------------------------------------------------------------------\n\nsimple = data[\"simple_rag\"]\nrl = data[\"rl_rag\"]\n\ndef extract_metric(results, key):\n    return np.array([r[key] for r in results])\n\nsimple_total = extract_metric(simple[\"detailed_results\"], \"total_reward\")\nrl_total = extract_metric(rl[\"detailed_results\"], \"total_reward\")\n\nmetrics = [\n    \"semantic_similarity\",\n    \"rouge_score\",\n    \"token_overlap\",\n    \"hallucination_score\",\n    \"context_relevance\"\n]\n\nsimple_metrics = {m: extract_metric(simple[\"detailed_results\"], m) for m in metrics}\nrl_metrics = {m: extract_metric(rl[\"detailed_results\"], m) for m in metrics}\n\nsamples = np.arange(1, len(simple_total) + 1)\n\n# -------------------------------------------------------------------------\n# 1) ZOOMED TOTAL REWARD COMPARISON\n# -------------------------------------------------------------------------\n\nplt.figure(figsize=(8,4.2))\nplt.plot(samples, simple_total, marker='o', label=\"Simple RAG\")\nplt.plot(samples, rl_total, marker='o', label=\"RL-RAG\")\n\nymin = min(simple_total.min(), rl_total.min()) - 0.02\nymax = max(simple_total.max(), rl_total.max()) + 0.02\nplt.ylim(ymin, ymax)\n\nplt.title(\"Per-sample Total Reward Comparison (Zoomed)\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Total Reward\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------------------\n# 2) DIFFERENCE PLOT (RL â€“ Simple)\n# -------------------------------------------------------------------------\n\nplt.figure(figsize=(8,3.6))\ndiff_total = rl_total - simple_total\nplt.plot(samples, diff_total, marker='o', color='purple')\n\nplt.axhline(0, linestyle='--', color='gray')\nplt.title(\"Difference (RL - Simple) in Total Reward\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Difference\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------------------\n# 3) AVERAGE METRIC COMPARISON (GROUPED BAR)\n# -------------------------------------------------------------------------\n\navg_simple = [simple_metrics[m].mean() for m in metrics]\navg_rl = [rl_metrics[m].mean() for m in metrics]\n\nx = np.arange(len(metrics))\nwidth = 0.35\n\nplt.figure(figsize=(9,4.2))\nplt.bar(x - width/2, avg_simple, width, label='Simple RAG')\nplt.bar(x + width/2, avg_rl, width, label='RL-RAG')\n\nplt.xticks(x, metrics, rotation=20)\nplt.title(\"Average Metric Comparison (Lower is Better for Hallucination Score)\")\nplt.ylabel(\"Average Value\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------------------\n# 4) METRIC DIFFERENCE HEATMAP\n# -------------------------------------------------------------------------\n\ndiff_matrix = np.vstack([rl_metrics[m] - simple_metrics[m] for m in metrics])\n\nplt.figure(figsize=(9,3.8))\nimg = plt.imshow(diff_matrix, aspect='auto', interpolation='nearest', cmap='viridis')\nplt.colorbar(img, label=\"RL - Simple\")\n\nplt.xticks(np.arange(len(samples)), samples)\nplt.yticks(np.arange(len(metrics)), metrics)\n\nplt.title(\"Per-sample Metric Differences (Positive = RL Higher)\")\nplt.xlabel(\"Sample Index\")\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------------------\n# 5) TRAINING REWARD HISTORY (IF AVAILABLE)\n# -------------------------------------------------------------------------\n\nif \"training_rewards\" in rl:\n    tr = np.array(rl[\"training_rewards\"])\n\n    plt.figure(figsize=(8,3.4))\n    plt.plot(np.arange(1, len(tr)+1), tr, marker='o', color='orange')\n    plt.title(\"RL-RAG Training Reward History\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Avg Reward\")\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport traceback\nfrom typing import Dict, List, Tuple\nimport numpy as np\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb import PersistentClient\nfrom peft import PeftModel\n\n# -----------------------\n# Configuration\n# -----------------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nLORA_DIR = \"/kaggle/input/generator/other/default/1/generator_lora_25k\"\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nQ_FILE = \"/kaggle/input/qvalue/q_values (2).json\"\nCHROMA_PATH = \"./.chroma\"\nCOLLECTION_NAME = \"rag_chunks\"\n\n# -----------------------\n# Load Models\n# -----------------------\nprint(f\"Loading models on {device}...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    base_model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\",\n    )\n    model = PeftModel.from_pretrained(base_model, LORA_DIR, use_bfloat16=False, use_bnb=False)\n    model.to(device)\n    model.eval()\n    print(\"âœ“ LoRA model loaded successfully!\")\nexcept Exception as e:\n    print(f\"âŒ Failed to load model: {e}\")\n    model = None\n    tokenizer = None\n\ntry:\n    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n    print(\"âœ“ Embedding model loaded!\")\nexcept Exception as e:\n    print(f\"âŒ Failed to load embedding model: {e}\")\n    embedding_model = None\n\n# -----------------------\n# Load/Create Chroma Vector DB\n# -----------------------\ntry:\n    chroma_client = PersistentClient(path=CHROMA_PATH)\n    # Check if collection exists\n    if COLLECTION_NAME in [c.name for c in chroma_client.list_collections()]:\n        collection = chroma_client.get_collection(name=COLLECTION_NAME)\n        print(f\"âœ“ Connected to existing Chroma collection '{COLLECTION_NAME}'\")\n    else:\n        collection = chroma_client.create_collection(name=COLLECTION_NAME, metadata={\"hnsw:space\": \"cosine\"})\n        print(f\"âœ“ Created new Chroma collection '{COLLECTION_NAME}'\")\nexcept Exception as e:\n    print(f\"âŒ Failed to initialize Chroma: {e}\")\n    collection = None\n\n# -----------------------\n# Load Pre-trained Q-values\n# -----------------------\ntry:\n    if os.path.exists(Q_FILE):\n        with open(Q_FILE, \"r\") as f:\n            Q_VALUES = json.load(f)\n        print(f\"âœ“ Loaded Q-values from {Q_FILE}\")\n        print(f\"   Q-values: {Q_VALUES}\")\n    else:\n        print(f\"âš ï¸  Q-values file '{Q_FILE}' not found! Using default values.\")\n        Q_VALUES = {\"rewrite_query\": 0.0, \"expand_context\": 0.0, \"filter_context\": 0.0, \"generate_response\": 0.0}\nexcept Exception as e:\n    print(f\"âŒ Failed to load Q-values: {e}\")\n    Q_VALUES = {\"rewrite_query\": 0.0, \"expand_context\": 0.0, \"filter_context\": 0.0, \"generate_response\": 0.0}\n\n# -----------------------\n# Helper Functions\n# -----------------------\ndef generate_text(prompt: str, max_new_tokens: int = 512, temperature: float = 0.7) -> str:\n    \"\"\"Generate text using the loaded model.\"\"\"\n    if model is None or tokenizer is None:\n        return \"MODEL_NOT_LOADED\"\n    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            top_p=0.95,\n            top_k=50,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if \"[/INST]\" in generated_text:\n        generated_text = generated_text.split(\"[/INST]\")[-1].strip()\n    return generated_text\n\ndef generate_embeddings_batch(chunks_batch: List[str]) -> List[List[float]]:\n    \"\"\"Generate embeddings for a batch of text chunks.\"\"\"\n    if embedding_model is None:\n        dim = 384\n        return [[0.0] * dim for _ in chunks_batch]\n    embeddings = embedding_model.encode(chunks_batch, convert_to_numpy=True)\n    return embeddings.tolist()\n\ndef generate_embeddings(chunks: List[str], batch_size: int = 32, verbose: bool = False) -> np.ndarray:\n    \"\"\"Generate embeddings for all chunks.\"\"\"\n    all_embeddings = []\n    if verbose:\n        print(f\"Generating embeddings for {len(chunks)} chunks...\")\n    for i in range(0, len(chunks), batch_size):\n        batch = chunks[i:i + batch_size]\n        embeddings = generate_embeddings_batch(batch)\n        all_embeddings.extend(embeddings)\n        if verbose and ((i + batch_size) % 100 == 0 or i + batch_size >= len(chunks)):\n            print(f\"Processed {min(i + batch_size, len(chunks))}/{len(chunks)} chunks\")\n    return np.array(all_embeddings)\n\ndef retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n    \"\"\"Retrieve relevant chunks from vector store.\"\"\"\n    if collection is None:\n        return []\n    try:\n        query_embedding = generate_embeddings([query_text])[0]\n        results = collection.query(\n            query_embeddings=[query_embedding.tolist()],\n            n_results=top_k\n        )\n        return results.get(\"documents\", [[]])[0]\n    except Exception as e:\n        print(f\"âŒ Error during retrieval: {e}\")\n        return []\n\ndef construct_prompt(query: str, context_chunks: List[str]) -> str:\n    context = \"\\n\".join(context_chunks[:2])\n    prompt = f\"\"\"You are a helpful coding assistant. Use the provided context to answer the question. If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\n--------------------------\nContext:\n{context}\n---------------------------\nQuestion: {query}\n\nAnswer:\"\"\"\n    return prompt\n\ndef generate_response(prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:\n    return generate_text(prompt, max_new_tokens=max_tokens, temperature=temperature)\n\ndef cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    if norm_vec1 == 0 or norm_vec2 == 0:\n        return 0.0\n    return float(dot_product / (norm_vec1 * norm_vec2))\n\ndef calculate_rouge_score(response: str, ground_truth: str) -> float:\n    \"\"\"Calculate ROUGE-L score.\"\"\"\n    def lcs_length(s1, s2):\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if s1[i-1] == s2[j-1]:\n                    dp[i][j] = dp[i-1][j-1] + 1\n                else:\n                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n        return dp[m][n]\n    \n    resp_tokens = response.lower().split()\n    gt_tokens = ground_truth.lower().split()\n    if len(resp_tokens) == 0 or len(gt_tokens) == 0:\n        return 0.0\n    lcs_len = lcs_length(resp_tokens, gt_tokens)\n    precision = lcs_len / len(resp_tokens) if len(resp_tokens) > 0 else 0\n    recall = lcs_len / len(gt_tokens) if len(gt_tokens) > 0 else 0\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndef calculate_token_overlap(response: str, ground_truth: str) -> float:\n    \"\"\"Calculate token overlap between response and ground truth.\"\"\"\n    resp_tokens = set(response.lower().split())\n    gt_tokens = set(ground_truth.lower().split())\n    if len(gt_tokens) == 0:\n        return 0.0\n    intersection = resp_tokens & gt_tokens\n    return len(intersection) / len(gt_tokens)\n\ndef detect_hallucination(response: str, context_chunks: List[str]) -> float:\n    \"\"\"Detect hallucination by checking if response terms are in context.\"\"\"\n    response_lower = response.lower()\n    context_combined = \" \".join(context_chunks).lower()\n    resp_terms = [w for w in response_lower.split() if len(w) > 3]\n    if len(resp_terms) == 0:\n        return 0.0\n    found_terms = sum(1 for term in resp_terms if term in context_combined)\n    coverage = found_terms / len(resp_terms)\n    return coverage\n\ndef calculate_context_relevance(query: str, context_chunks: List[str]) -> float:\n    \"\"\"Calculate relevance of context to query.\"\"\"\n    if not context_chunks:\n        return 0.0\n    query_embedding = generate_embeddings([query])[0]\n    context_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n    similarities = [\n        np.dot(query_embedding, ctx_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(ctx_emb) + 1e-12)\n        for ctx_emb in context_embeddings\n    ]\n    return float(np.mean(similarities)) if similarities else 0.0\n\ndef calculate_hybrid_reward(\n    response: str,\n    ground_truth: str,\n    query: str,\n    context_chunks: List[str],\n    weights: Dict[str, float] = None\n) -> Dict[str, float]:\n    \"\"\"Calculate comprehensive reward metrics.\"\"\"\n    if weights is None:\n        weights = {\n            'semantic_similarity': 0.35,\n            'rouge_score': 0.25,\n            'token_overlap': 0.15,\n            'hallucination_penalty': 0.15,\n            'context_relevance': 0.10\n        }\n    \n    response_embedding = generate_embeddings([response])[0]\n    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n    semantic_sim = cosine_similarity(response_embedding, ground_truth_embedding)\n    rouge_score = calculate_rouge_score(response, ground_truth)\n    token_overlap = calculate_token_overlap(response, ground_truth)\n    hallucination_score = detect_hallucination(response, context_chunks)\n    context_relevance = calculate_context_relevance(query, context_chunks)\n    \n    hybrid_reward = (\n        weights['semantic_similarity'] * semantic_sim +\n        weights['rouge_score'] * rouge_score +\n        weights['token_overlap'] * token_overlap +\n        weights['hallucination_penalty'] * hallucination_score +\n        weights['context_relevance'] * context_relevance\n    )\n    \n    return {\n        'total_reward': hybrid_reward,\n        'semantic_similarity': semantic_sim,\n        'rouge_score': rouge_score,\n        'token_overlap': token_overlap,\n        'hallucination_score': hallucination_score,\n        'context_relevance': context_relevance\n    }\n\n# -----------------------\n# Data Loading\n# -----------------------\ndef load_glaive_dataset(file_path: str, num_samples: int = 1000) -> List[Dict[str, str]]:\n    \"\"\"Load Glaive dataset from JSON file.\"\"\"\n    print(f\"Loading Glaive dataset from {file_path}...\")\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    random.shuffle(data)\n    data = data[:num_samples]\n    print(f\"Loaded {len(data)} samples from Glaive dataset\")\n    return data\n\ndef create_document_chunks_from_glaive(data: List[Dict[str, str]]) -> List[str]:\n    \"\"\"Create document chunks from Glaive data.\"\"\"\n    chunks = []\n    for idx, item in enumerate(data, 1):\n        question = item.get('question', '')\n        answer = item.get('answer', '')\n        chunk = f\"<id:{idx}>\\n{question}\\n{answer}\\n</id:{idx}>\"\n        chunks.append(chunk)\n    print(f\"Created {len(chunks)} document chunks for RAG retrieval\")\n    return chunks\n\ndef preprocess_text(text: str) -> str:\n    \"\"\"Preprocess text to lowercase.\"\"\"\n    return text.lower()\n\ndef preprocess_chunks(chunks: List[str]) -> List[str]:\n    \"\"\"Preprocess all chunks.\"\"\"\n    return [preprocess_text(chunk) for chunk in chunks]\n\ndef add_to_vector_store(embeddings: np.ndarray, chunks: List[str]):\n    \"\"\"Add embeddings and chunks to Chroma vector store.\"\"\"\n    if collection is None:\n        print(\"âŒ Collection not initialized, cannot add to vector store\")\n        return\n    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n    try:\n        collection.add(\n            ids=ids,\n            embeddings=embeddings.tolist(),\n            documents=chunks\n        )\n        print(f\"âœ“ Added {len(chunks)} chunks to Chroma collection '{COLLECTION_NAME}'\")\n    except Exception as e:\n        print(f\"âŒ Error adding to vector store: {e}\")\n        traceback.print_exc()\n\n# -----------------------\n# Basic RAG Pipeline\n# -----------------------\ndef basic_rag_pipeline(query: str) -> tuple:\n    \"\"\"Execute basic RAG without RL.\"\"\"\n    relevant_chunks = retrieve_relevant_chunks(query)\n    prompt = construct_prompt(query, relevant_chunks)\n    response = generate_response(prompt)\n    return response, relevant_chunks\n\n# -----------------------\n# RL-Enhanced RAG Actions\n# -----------------------\ndef rewrite_query(query: str, context_chunks: List[str]) -> str:\n    \"\"\"Rewrite query for better retrieval.\"\"\"\n    rewrite_prompt = f\"\"\"You are a query optimization assistant. Rewrite the given query to make it more effective for retrieving relevant information.\n\nOriginal query: {query}\n\nContext so far:\n{' '.join(context_chunks[:2]) if context_chunks else 'No context available yet'}\n\nProvide ONLY the rewritten query, nothing else. Make it more specific and targeted for better document retrieval.\n\nRewritten query:\"\"\"\n    rewritten_query = generate_text(rewrite_prompt, max_new_tokens=100, temperature=0.3)\n    return rewritten_query.strip()\n\ndef expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n    \"\"\"Expand context with additional chunks.\"\"\"\n    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n    new_chunks = [chunk for chunk in additional_chunks if chunk not in current_chunks]\n    expanded_context = current_chunks + new_chunks[:top_k]\n    return expanded_context\n\ndef filter_context(query: str, context_chunks: List[str]) -> List[str]:\n    \"\"\"Filter context to keep only most relevant chunks.\"\"\"\n    if not context_chunks:\n        return []\n    query_embedding = generate_embeddings([query])[0]\n    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n    relevance_scores = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunk_embeddings]\n    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n    return filtered_chunks\n\ndef select_best_action() -> str:\n    \"\"\"Select action with highest Q-value (greedy policy for inference).\"\"\"\n    return max(Q_VALUES, key=Q_VALUES.get)\n\ndef rl_enhanced_rag_pipeline(query: str, max_steps: int = 5) -> tuple:\n    \"\"\"Execute RL-enhanced RAG using pre-trained Q-values.\"\"\"\n    # Initialize state\n    context_chunks = retrieve_relevant_chunks(query)\n    current_query = query\n    actions_taken = []\n    \n    for step in range(max_steps):\n        action = select_best_action()\n        actions_taken.append(action)\n        \n        if action == \"rewrite_query\":\n            current_query = rewrite_query(current_query, context_chunks)\n            context_chunks = retrieve_relevant_chunks(current_query)\n            \n        elif action == \"expand_context\":\n            context_chunks = expand_context(current_query, context_chunks)\n            \n        elif action == \"filter_context\":\n            context_chunks = filter_context(current_query, context_chunks)\n            \n        elif action == \"generate_response\":\n            prompt = construct_prompt(current_query, context_chunks)\n            response = generate_response(prompt)\n            return response, context_chunks, actions_taken\n    \n    # If we exit loop without generating, generate response\n    prompt = construct_prompt(current_query, context_chunks)\n    response = generate_response(prompt)\n    return response, context_chunks, actions_taken\n\n# -----------------------\n# Inference Function\n# -----------------------\ndef run_inference(query: str, ground_truth: str = None):\n    \"\"\"Run inference comparing normal RAG vs RL-enhanced RAG.\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RAG INFERENCE - COMPARISON\")\n    print(\"=\" * 80)\n    print(f\"\\nQuery: {query}\\n\")\n    \n    # Normal RAG\n    print(\"ğŸ”µ Running Normal RAG...\")\n    normal_response, normal_context = basic_rag_pipeline(query)\n    print(f\"\\nNormal RAG Response:\\n{normal_response}\\n\")\n    \n    # RL-Enhanced RAG\n    print(\"ğŸŸ¢ Running RL-Enhanced RAG...\")\n    rl_response, rl_context, actions_taken = rl_enhanced_rag_pipeline(query)\n    print(f\"\\nRL-Enhanced RAG Response:\\n{rl_response}\\n\")\n    print(f\"Actions taken: {' â†’ '.join(actions_taken)}\\n\")\n    \n    # Calculate rewards if ground truth provided\n    if ground_truth:\n        print(\"ğŸ“Š Calculating Reward Scores...\")\n        normal_metrics = calculate_hybrid_reward(normal_response, ground_truth, query, normal_context)\n        rl_metrics = calculate_hybrid_reward(rl_response, ground_truth, query, rl_context)\n        \n        print(\"\\n\" + \"-\" * 80)\n        print(\"REWARD SCORES\")\n        print(\"-\" * 80)\n        print(f\"\\nğŸ”µ Normal RAG Total Reward: {normal_metrics['total_reward']:.4f}\")\n        print(f\"   - Semantic Similarity: {normal_metrics['semantic_similarity']:.4f}\")\n        print(f\"   - ROUGE Score: {normal_metrics['rouge_score']:.4f}\")\n        print(f\"   - Token Overlap: {normal_metrics['token_overlap']:.4f}\")\n        print(f\"   - Hallucination Score: {normal_metrics['hallucination_score']:.4f}\")\n        print(f\"   - Context Relevance: {normal_metrics['context_relevance']:.4f}\")\n        \n        print(f\"\\nğŸŸ¢ RL-Enhanced RAG Total Reward: {rl_metrics['total_reward']:.4f}\")\n        print(f\"   - Semantic Similarity: {rl_metrics['semantic_similarity']:.4f}\")\n        print(f\"   - ROUGE Score: {rl_metrics['rouge_score']:.4f}\")\n        print(f\"   - Token Overlap: {rl_metrics['token_overlap']:.4f}\")\n        print(f\"   - Hallucination Score: {rl_metrics['hallucination_score']:.4f}\")\n        print(f\"   - Context Relevance: {rl_metrics['context_relevance']:.4f}\")\n        \n        improvement = ((rl_metrics['total_reward'] - normal_metrics['total_reward']) / \n                      normal_metrics['total_reward'] * 100) if normal_metrics['total_reward'] > 0 else 0\n        print(f\"\\nğŸ“ˆ Improvement: {improvement:+.2f}%\")\n        print(\"-\" * 80)\n        \n        return {\n            'query': query,\n            'ground_truth': ground_truth,\n            'normal_rag': {\n                'response': normal_response,\n                'context_chunks': normal_context,\n                'metrics': normal_metrics\n            },\n            'rl_rag': {\n                'response': rl_response,\n                'context_chunks': rl_context,\n                'actions_taken': actions_taken,\n                'metrics': rl_metrics\n            },\n            'improvement_percentage': improvement\n        }\n    else:\n        print(\"â„¹ï¸  No ground truth provided - skipping reward calculation\")\n        return {\n            'query': query,\n            'normal_rag': {'response': normal_response, 'context_chunks': normal_context},\n            'rl_rag': {'response': rl_response, 'context_chunks': rl_context, 'actions_taken': actions_taken}\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:08:27.948414Z","iopub.execute_input":"2025-11-18T04:08:27.948730Z","iopub.status.idle":"2025-11-18T04:10:25.908656Z","shell.execute_reply.started":"2025-11-18T04:08:27.948703Z","shell.execute_reply":"2025-11-18T04:10:25.907950Z"}},"outputs":[{"name":"stderr","text":"2025-11-18 04:08:38.293688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763438918.535460      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763438918.595485      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading models on cuda...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b034bfb2a273454aa655a694646a7652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992fb68617f043d89c7cb1e8b50dd7dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5099fcce965d4bfc949d81834eff6e76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e6cddf73ef24565affc3ccc143dc00b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b4bace6758041abb8c7ab382a90c5e7"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4849b5c17cba4d078e0a03d2fa3d9280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d364eeef57794ddb9330b32203353906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8974f4dab9f468eafc040bba427612b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e874ecf8b3ea4008a9bdb4ef42ecb802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bb80d1f7804abbb667a045282d82d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ee52f17e764c00990d8cf24c8fa096"}},"metadata":{}},{"name":"stdout","text":"âœ“ LoRA model loaded successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088d0186878b4dcd95db41ddfb7f7839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c223768c9e4dd3ab82a1df7de3a604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d136638e67046abbf216455b66d418b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347ce4e8ae13403883cc95bf4ba6e5ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f922f39217e748aca49935047754fae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b34c1dec9774df785cfefe10244dbc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef88525fa55413b837dfa12128679ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881f52e5d5e64ce0a2e611ef8bdcf872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c7e0a4ce2e4acca717af24024555b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86cd3417494449358c5c3da52529c751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a1cdaf72dd4ef98c02241929ee6535"}},"metadata":{}},{"name":"stdout","text":"âœ“ Embedding model loaded!\nâœ“ Connected to existing Chroma collection 'rag_chunks'\nâœ“ Loaded Q-values from /kaggle/input/qvalue/q_values (2).json\n   Q-values: {'rewrite_query': 0.18677308480050683, 'expand_context': 0.051585000000000006, 'filter_context': 0.08103826938353614, 'generate_response': 0.5009191809725395}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -----------------------\n# Main Execution\n# -----------------------\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RAG WITH RL - SETUP AND INFERENCE\")\nprint(\"=\" * 80)\n\nglaive_file_path = \"/kaggle/input/glaive-coding-assistant/c9bc9129-eba0-4b10-8292-4ae70fc7fa0d.json\"\n\n\n# Load dataset and create vector store\nprint(\"\\n[STEP 1] Loading Dataset and Creating Vector Store...\")\nglaive_data = load_glaive_dataset(glaive_file_path, num_samples=1000)\n\n# Create chunks\ndocument_chunks = create_document_chunks_from_glaive(glaive_data)\npreprocessed_chunks = preprocess_chunks(document_chunks)\n\n# Generate embeddings\nprint(\"\\n[STEP 2] Generating Embeddings...\")\nembeddings = generate_embeddings(preprocessed_chunks, verbose=True)\n\n# Add to vector store\nprint(\"\\n[STEP 3] Adding to Vector Store...\")\nadd_to_vector_store(embeddings, preprocessed_chunks)\n\nprint(\"\\nâœ“ Vector store setup complete!\\n\")\n\n# Save the Q&A data for reference\ntry:\n    with open(\"glaive_1000_qas.txt\", \"w\", encoding=\"utf-8\") as out:\n        for i, item in enumerate(glaive_data, 1):\n            q = item.get(\"question\", \"\").strip()\n            a = item.get(\"answer\", \"\").strip()\n            out.write(f\"### Sample {i}\\nQuestion:\\n{q}\\n\\nAnswer:\\n{a}\\n\\n\\n\")\n    print(\"âœ“ Saved 1000 Q/A to glaive_1000_qas.txt\\n\")\nexcept Exception as e:\n    print(f\"âš ï¸  Warning: could not save glaive_1000_qas.txt: {e}\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:10:34.550069Z","iopub.execute_input":"2025-11-18T04:10:34.550358Z","iopub.status.idle":"2025-11-18T04:10:40.846116Z","shell.execute_reply.started":"2025-11-18T04:10:34.550336Z","shell.execute_reply":"2025-11-18T04:10:40.845463Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRAG WITH RL - SETUP AND INFERENCE\n================================================================================\n\n[STEP 1] Loading Dataset and Creating Vector Store...\nLoading Glaive dataset from /kaggle/input/glaive-coding-assistant/c9bc9129-eba0-4b10-8292-4ae70fc7fa0d.json...\nLoaded 1000 samples from Glaive dataset\nCreated 1000 document chunks for RAG retrieval\n\n[STEP 2] Generating Embeddings...\nGenerating embeddings for 1000 chunks...\nProcessed 800/1000 chunks\nProcessed 1000/1000 chunks\n\n[STEP 3] Adding to Vector Store...\nâœ“ Added 1000 chunks to Chroma collection 'rag_chunks'\n\nâœ“ Vector store setup complete!\n\nâœ“ Saved 1000 Q/A to glaive_1000_qas.txt\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Now run inference examples\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING INFERENCE EXAMPLES\")\nprint(\"=\" * 80)\n\n# Example 1: With ground truth\nquery1 = \"How do I create a list comprehension in Python?\"\nground_truth1 = \"List comprehensions in Python provide a concise way to create lists. The syntax is [expression for item in iterable if condition]. For example: squares = [x**2 for x in range(10)] creates a list of squares.\"\n\nresults1 = run_inference(query1)\n\n# Example 2: From actual dataset\nprint(\"\\n\\n\" + \"=\" * 80)\nprint(\"EXAMPLE 2: Using sample from dataset\")\nprint(\"=\" * 80)\n\nsample = random.choice(glaive_data)\nquery2 = sample['question']\nground_truth2 = sample['answer']\n\nresults2 = run_inference(query2)\n\n# Save results\ntry:\n    all_results = {\n        'example_1': results1,\n        'example_2': results2\n    }\n    with open('inference_results.json', 'w') as f:\n        json.dump(all_results, f, indent=2, default=str)\n    print(\"\\nâœ“ Results saved to 'inference_results.json'\")\nexcept Exception as e:\n    print(f\"\\nâš ï¸  Failed to save results: {e}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SETUP AND INFERENCE COMPLETE!\")\nprint(\"=\" * 80)\nprint(\"\\nYou can now use run_inference() with your own queries:\")\nprint(\"  run_inference('your query here', 'optional ground truth')\")\nprint(\"=\" * 80)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:15:50.079520Z","iopub.execute_input":"2025-11-18T03:15:50.079885Z","iopub.status.idle":"2025-11-18T03:17:12.771547Z","shell.execute_reply.started":"2025-11-18T03:15:50.079863Z","shell.execute_reply":"2025-11-18T03:17:12.770169Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n================================================================================\nRUNNING INFERENCE EXAMPLES\n================================================================================\n\n================================================================================\nRAG INFERENCE - COMPARISON\n================================================================================\n\nQuery: How do I create a list comprehension in Python?\n\nğŸ”µ Running Normal RAG...\n\nNormal RAG Response:\n[INST] You are a helpful coding assistant. Use the provided context to answer the question. If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\n\nContext:\n<id:560>\ni have a table called \"employees\" and i want to find the age of the oldest employee. how can i write a sql query to achieve this?\nto find the age of the oldest employee in the \"employees\" table, you can use the following sql query:\n\n```sql\nselect max(age) from employees;\n```\n\nthis query selects the maximum (oldest) value from the \"age\" column in the \"employees\" table. the max() function returns the largest value within a set, which in this case is the oldest age. by executing this query, you will obtain the age of the oldest employee in the table.\n</id:560>\n<id:580>\nin python 3.4, the new module enum has been introduced which allows to add an enumerated type to the language. when working with this new module, i have encountered a scenario where i need to define a class constant inside an enum. however, whenever i try to do that, it simply gets added as one of the members of the enum, which is not what i want. take the following example code:\n\n```python\nclass planet(enum):\n    mercury = (3.303e+23, 2.4397e6)\n    venus   = (4.869e+24, 6.0518e6)\n    earth   = (5.976e+24, 6.37814e6)\n    mars    = (6.421e+23, 3.3972e6)\n    jupiter = (1.9e+27,   7.1492e7)\n    saturn  = (5.688e+26, 6.0268e7)\n    uranus  = (8.686e+25, 2.5559e7)\n    neptune = (1.024e+26, 2.4746e7)\n    def __init__(self, mass, radius):\n        self.mass = mass       # in kilograms\n        self.radius = radius   # in meters\n    @property\n    def surface_gravity(self):\n        # universal gravitational constant  (m3 kg-1 s-2)\n        g = 6.67300e-11\n        return g * self.mass / (self.radius * self.radius)\n```\n\nin the method 'surface_gravity', i have a constant 'g' which i would normally define at the class level. if i wanted to use this constant in other methods, i would have to define it there as well which is not ideal. is there any way to define a class constant inside an enum, or some other workaround to achieve the same effect?\nyes, it is indeed possible to define a class constant inside an enum in python. the trick is to use the _ignore_ attribute which is a list of names (strings) which are not transformed into enum members. you can define your constant there. here's an example of how you can define a constant in your enum class:\n\n```python\nclass planet(enum):\n    _ignore_ = ['g']\n    g = 6.67300e-11\n    mercury = (3.303e+23, 2.4397e6)\n    venus   = (4.869e+24, 6.0518e6)\n    earth   = (5.976e+24, 6.37814e6)\n    mars    = (6.421e+23, 3.3972e6)\n    jupiter = (1.9e+27,   7.1492e7)\n    saturn  = (5.688e+26, 6.0268e7)\n    uranus  = (8.686e+25, 2.5559e7)\n    neptune = (1.024e+26, 2.4746e7)\n    def __init__(self, mass, radius):\n        self.mass = mass       # in kilograms\n        self.radius = radius   # in meters\n    @property\n    def surface_gravity(self):\n        return self.g * self.mass / (self.radius * self.radius)\n```\n\nin this code, the _ignore_ attribute is used to prevent the enum class from considering 'g' as an enum member. as a result, 'g' can be accessed as a normal class attribute.\n</id:580>\n<id:791>\ndo python function annotations and type hints, as defined in pep 3107 and pep 484, cause any run-time effects? could these annotations potentially speed up the code, reduce memory usage, or conversely slow down the code execution?\npython function annotations, also known as type hints, do not have any run-time effects. they are purely optional metadata information about the types used by your functions. \n\nthe python interpreter does not enforce type checking based on these annotations. they are used by certain tools, ides and linters for improved code intelligence and static type checking. therefore, using type hints won't make your python code run faster or slower, nor will they affect memory usage.\n\nhere's an example of function annotations in python:\n\n```python\ndef greet(name: str) -> str:\n    return 'hello, ' + name\n```\n\nin this function, `name: str` is an annotation indicating that the `name` argument should be a string, and `-> str` is an annotation indicating the function should return a string.\n\nif you're interested in static type checking in python, you might want to look into tools like `mypy` or `pyright`, which can analyze your code based on these annotations and help catch certain types of bugs before runtime.\n\nremember that python's dynamic typing is one of its strengths, and type annotations should be used as a tool for clarity and bug prevention, not as a mechanism to transform python into a statically-typed language.\n</id:791>\n<id:835>\ni am encountering an issue in python where the list.append() function becomes progressively slower as the list grows while reading from a large file. i am looping through a file, converting every few lines to an instance of an object and appending the instance to a list. i've noticed that as the list grows, the looping slows down progressively. this doesn't seem to be an issue with the loop, because when i print every new instance as i loop through the file, the program progresses at a constant speed. it only slows down when i append the instances to the list. a friend suggested disabling garbage collection before the loop and enabling it afterward, but i'm not sure if this is the best solution. has anyone else observed this issue with list.append getting slower? what is the best way to pre-allocate memory or use deque to circumvent this issue?\nthe python list.append() method shouldn't be slowing down as the list grows, assuming there's enough memory available to accommodate the growing list. python's list is implemented as a dynamic array, which means it pre-allocates more space than it needs to accommodate for growth. however, when this pre-allocated space is filled up, python needs to allocate a larger block of memory, copy the old data, and release the old memory. this operation is rare, but it can slow down your program if it happens frequently.\n\nhere are a couple of solutions to your problem.\n\n1. disabling and enabling garbage collection:\n\n    python's garbage collector can sometimes interfere with your program and slow it down. you can try disabling garbage collection before your loop and enabling it afterward.\n\n    here's how you can do it:\n\n    ```python\n    import gc\n\n    gc.disable()\n    # your loop here.\n    gc.enable()\n    ```\n\n    you can also call gc.collect() after enabling the garbage collector to immediately collect all unreferenced objects.\n\n2. pre-allocating memory:\n\n    if you know how many elements you're going to append to the list in advance, you can pre-allocate memory for the list. this way, python won't need to allocate more memory as the list grows.\n\n    ```python\n    your_list = [none] * number_of_elements\n    ```\n\n    then, instead of using list.append(), you can use list.extend() to append multiple elements at once.\n\n    ```python\n    your_list.extend(list_of_elements)\n    ```\n\n3. using deque:\n\n    if you're dealing with a large number of elements, you can use a deque instead of a list. a deque is a double-ended queue that provides fast appends and pops from either end.\n\n    ```python\n    from collections import deque\n\n    your_deque = deque(maxlen=number_of_elements)\n    your_deque.extend(list_of_elements)\n    ```\n\n    deque has a maxlen parameter that determines the maximum size of the deque. it will automatically grow and shrink as needed, but it will never exceed the maxlen limit.\n\n    hope this helps!\n</id:835>\n<id:839>\ni'm trying to set the column widths in a pandas dataframe to a fixed width. i've tried using the pd.set_option function with the 'display.width_fix' option set to 'True', but it's not working. can you please help me figure out how to fix this?\nthe `pd.set_option()` function is used to set the pandas options globally. the `display.width_fix` option is used to set the width of the table output to the fixed width. however, this option is only used by the `pandas.DataFrame.to_string()` method and it doesn't have any effect on other methods like `pandas.DataFrame.to_html()`.\n\nif you want to set the column widths to a fixed width, you can use the `pandas.DataFrame.style.format()` method with the `width` parameter. here's an example:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})\n\n# set the column widths to 5 characters\ndf.style.format(width = 5)\n```\n\nthis will display the table with the column widths set to 5 characters.\n\nhope\n\nğŸŸ¢ Running RL-Enhanced RAG...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2591268313.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mground_truth1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"List comprehensions in Python provide a concise way to create lists. The syntax is [expression for item in iterable if condition]. For example: squares = [x**2 for x in range(10)] creates a list of squares.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Example 2: From actual dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/50000937.py\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(query, ground_truth)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# RL-Enhanced RAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸŸ¢ Running RL-Enhanced RAG...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mrl_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_enhanced_rag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nRL-Enhanced RAG Response:\\n{rl_response}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Actions taken: {' â†’ '.join(actions_taken)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/50000937.py\u001b[0m in \u001b[0;36mrl_enhanced_rag_pipeline\u001b[0;34m(query, max_steps)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"generate_response\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/50000937.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, query, max_tokens, temperature)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;34m\"\"\"Generate a response from the LLM and extract clean answer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mclean_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/50000937.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"import gradio as gr\nimport json\n\ndef format_metrics(metrics):\n    \"\"\"Format metrics dictionary into readable string.\"\"\"\n    if not metrics:\n        return \"No metrics available\"\n    \n    return f\"\"\"\n**Total Reward:** {metrics.get('total_reward', 0):.4f}\n- Semantic Similarity: {metrics.get('semantic_similarity', 0):.4f}\n- ROUGE Score: {metrics.get('rouge_score', 0):.4f}\n- Token Overlap: {metrics.get('token_overlap', 0):.4f}\n- Hallucination Score: {metrics.get('hallucination_score', 0):.4f}\n- Context Relevance: {metrics.get('context_relevance', 0):.4f}\n\"\"\"\n\ndef format_context_chunks(chunks):\n    \"\"\"Format context chunks into readable string.\"\"\"\n    if not chunks:\n        return \"No context chunks available\"\n    \n    formatted = \"\"\n    for i, chunk in enumerate(chunks, 1):\n        formatted += f\"**Chunk {i}:**\\n{chunk}\\n\\n\"\n    return formatted\n\ndef process_query(query, ground_truth=None):\n    \"\"\"Process query and return formatted results.\"\"\"\n    # Call your run_inference function\n    results = run_inference(query, ground_truth)\n    \n    # Extract data\n    normal_rag = results.get('normal_rag', {})\n    rl_rag = results.get('rl_rag', {})\n    \n    # Format outputs\n    normal_response = normal_rag.get('response', 'No response available')\n    normal_context = format_context_chunks(normal_rag.get('context_chunks', []))\n    normal_metrics = format_metrics(normal_rag.get('metrics'))\n    \n    rl_response = rl_rag.get('response', 'No response available')\n    rl_context = format_context_chunks(rl_rag.get('context_chunks', []))\n    rl_actions = \", \".join(rl_rag.get('actions_taken', [])) if rl_rag.get('actions_taken') else \"No actions recorded\"\n    rl_metrics = format_metrics(rl_rag.get('metrics'))\n    \n    improvement = results.get('improvement_percentage', 0)\n    improvement_text = f\"**Improvement:** {improvement:+.2f}%\" if ground_truth else \"No ground truth provided\"\n\n    \n    \n    return (\n        normal_response,\n        normal_context,\n        normal_metrics,\n        rl_response,\n        rl_context,\n        rl_actions,\n        rl_metrics,\n        improvement_text\n    )\n\n# Create Gradio interface\nwith gr.Blocks(title=\"RAG Comparison Interface\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# ğŸ” RAG Inference Comparison\")\n    gr.Markdown(\"Compare Normal RAG vs RL-Enhanced RAG performance\")\n    \n    with gr.Row():\n        with gr.Column():\n            query_input = gr.Textbox(\n                label=\"Query\",\n                placeholder=\"Enter your question here...\",\n                lines=3\n            )\n            ground_truth_input = gr.Textbox(\n                label=\"Ground Truth (Optional)\",\n                placeholder=\"Enter expected answer for metrics calculation...\",\n                lines=3\n            )\n            submit_btn = gr.Button(\"Run Inference\", variant=\"primary\", size=\"lg\")\n    \n    gr.Markdown(\"---\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            gr.Markdown(\"## ğŸ”µ Normal RAG\")\n            \n            gr.Markdown(\"### Response\")\n            normal_response_output = gr.Textbox(\n                label=\"\",\n                lines=6,\n                interactive=False,\n                show_label=False\n            )\n            \n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Context Chunks\")\n            normal_context_output = gr.Markdown()\n            \n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Metrics\")\n            normal_metrics_output = gr.Markdown()\n        \n        with gr.Column(scale=1):\n            gr.Markdown(\"## ğŸŸ¢ RL-Enhanced RAG\")\n            \n            gr.Markdown(\"### Response\")\n            rl_response_output = gr.Textbox(\n                label=\"\",\n                lines=6,\n                interactive=False,\n                show_label=False\n            )\n            \n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Context Chunks\")\n            rl_context_output = gr.Markdown()\n            \n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Actions Taken\")\n            rl_actions_output = gr.Textbox(\n                label=\"\",\n                lines=2,\n                interactive=False,\n                show_label=False\n            )\n            \n            gr.Markdown(\"---\")\n            gr.Markdown(\"### Metrics\")\n            rl_metrics_output = gr.Markdown()\n    \n    gr.Markdown(\"---\")\n    \n    with gr.Row():\n        improvement_output = gr.Markdown(label=\"Improvement\")\n    \n    # Connect the button to the processing function\n    submit_btn.click(\n        fn=process_query,\n        inputs=[query_input, ground_truth_input],\n        outputs=[\n            normal_response_output,\n            normal_context_output,\n            normal_metrics_output,\n            rl_response_output,\n            rl_context_output,\n            rl_actions_output,\n            rl_metrics_output,\n            improvement_output\n        ]\n    )\n    \n    # Example queries\n    gr.Markdown(\"---\")\n    gr.Markdown(\"### ğŸ’¡ Example Queries\")\n    examples = gr.Examples(\n        examples=[\n            [\"What is machine learning?\", \"Machine learning is a subset of AI that enables systems to learn from data.\"],\n            [\"Explain reinforcement learning\", \"\"],\n            [\"What are the benefits of RAG systems?\", \"\"]\n        ],\n        inputs=[query_input, ground_truth_input]\n    )\n\n# Launch the interface\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T04:10:56.519326Z","iopub.execute_input":"2025-11-18T04:10:56.519636Z","iopub.status.idle":"2025-11-18T04:11:02.460939Z","shell.execute_reply.started":"2025-11-18T04:10:56.519613Z","shell.execute_reply":"2025-11-18T04:11:02.460074Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://bfb7da42f18cb763e6.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://bfb7da42f18cb763e6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nRAG INFERENCE - COMPARISON\n================================================================================\n\nQuery: How do I create a list comprehension in Python?\n\nğŸ”µ Running Normal RAG...\n\nNormal RAG Response:\nYou can create a list comprehension in Python by using the following syntax:\n\n```python\n[expression for item in sequence]\n```\n\nIn this syntax, 'expression' is a computation that is applied to each element in 'sequence', and the resulting value is added to the list. The 'item' variable represents the current element being iterated over in the sequence.\n\nHere's an example of how you can use a list comprehension to create a list of squares:\n\n```python\nsquares = [x**2 for x in range(1, 11)]\nprint(squares)  # Output: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n```\n\nIn this code, the list comprehension is defined as `[x**2 for x in range(1, 11)]`. The expression `x**2` is applied to each element in the sequence `range(1, 11)`, which results in a list of squares from 1 to 10.\n\nList comprehensions can be a useful tool for creating lists in Python, especially when you need to perform operations on each element in a sequence.\n\nğŸŸ¢ Running RL-Enhanced RAG...\n\nRL-Enhanced RAG Response:\nA list comprehension is a concise way to create a list in Python. It involves using the square brackets and for-loop syntax to generate a list. Here is an example of how to create a list comprehension in Python:\n\n```python\n[x*x for x in range(10)]\n```\n\nIn this example, the code generates a list of squares from 0 to 9. The `range(10)` function generates a list of numbers from 0 to 9. The `x*x` part calculates the square of each number. Finally, the square brackets `[]` create the list comprehension.\n\nHere is a more detailed explanation of the code:\n\n```python\n[x*x for x in range(10)]\n```\n\nThe `range(10)` function generates a list of numbers from 0 to 9. The `x*x` part calculates the square of each number. The square brackets `[]` create the list comprehension.\n\nThe resulting list is: `[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]`\n\nActions taken: generate_response\n\nâ„¹ï¸  No ground truth provided - skipping reward calculation\n\n================================================================================\nRAG INFERENCE - COMPARISON\n================================================================================\n\nQuery: How do I create a list comprehension in Python?\n\nğŸ”µ Running Normal RAG...\n\nNormal RAG Response:\nTo create a list comprehension in Python, you can use the following syntax:\n\n```python\n[expr for item in iterable if condition]\n```\n\nIn this syntax, `expr` is an expression that is evaluated for each item in the iterable `iterable`. `condition` is an optional condition that is used to filter the items in `iterable` before the expression is evaluated.\n\nHere's an example of how you can use a list comprehension to square all the numbers in a list:\n\n```python\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x * x for x in numbers]\nprint(squares)  # Output: [1, 4, 9, 16, 25]\n```\n\nIn this code, `numbers` is the iterable and `x` is the item being iterated over. The expression `x * x` is evaluated for each item `x` in `numbers`, and the resulting values are stored in the list `squares`. The square of each number is computed using the multiplication operator `*`.\n\nYou can also use a condition to filter the items in the iterable before the expression is evaluated. For example, you can use the condition `x > 3` to only square the numbers greater than 3:\n\n```python\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x * x for x in numbers if x > 3]\nprint(squares)  # Output: [9, 16, 25]\n```\n\nIn this code, the list comprehension only squares the numbers greater than 3 by using the condition `x > 3`. The resulting list `squares` only contains the squares of numbers greater than 3.\n\nYou can also use multiple conditions in a list comprehension by separating them with the `and` operator. For example, you can use the conditions `x > 3` and `x < 7` to only square the numbers between 3 and 7:\n\n```python\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x * x for x in numbers if x > 3 and x < 7]\nprint(squares)  # Output: [9, 16]\n```\n\nIn this code, the list comprehension only squares the numbers between 3 and 7 by using the conditions `x > 3` and `x < 7`. The resulting list `squares` only contains the squares of numbers between 3 and 7.\n\nOverall, list comprehensions provide a concise and efficient way to create lists in Python. They allow you to apply a transformation to each item in an iterable and store the resulting values in a new list.\n\nğŸŸ¢ Running RL-Enhanced RAG...\n\nRL-Enhanced RAG Response:\nYou can use the following code to create a list comprehension in Python:\n\n```python\nnums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nsquares = [num**2 for num in nums]\n```\n\nIn this code, the `nums` variable is a list of numbers. The `squares` variable is a list comprehension that creates a new list called `squares` from the `nums` list. The syntax for a list comprehension in Python is `[expression for item in iterable]`.\n\nIn this case, the `expression` is `num**2` which calculates the square of each number in the `nums` list. The `item` is the current element being iterated over in the `nums` list. The `iterable` is the `nums` list itself.\n\nAs a result, the `squares` variable will contain a list of the squares of each number in the `nums` list.\n\nActions taken: generate_response\n\nğŸ“Š Calculating Reward Scores...\n\n--------------------------------------------------------------------------------\nREWARD SCORES\n--------------------------------------------------------------------------------\n\nğŸ”µ Normal RAG Total Reward: 0.5270\n   - Semantic Similarity: 0.8716\n   - ROUGE Score: 0.0882\n   - Token Overlap: 0.6552\n   - Hallucination Score: 0.5058\n   - Context Relevance: 0.2572\n\nğŸŸ¢ RL-Enhanced RAG Total Reward: 0.5276\n   - Semantic Similarity: 0.8398\n   - ROUGE Score: 0.1529\n   - Token Overlap: 0.5172\n   - Hallucination Score: 0.6143\n   - Context Relevance: 0.2572\n\nğŸ“ˆ Improvement: +0.12%\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport gc\n\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.synchronize()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T03:30:08.984253Z","iopub.execute_input":"2025-11-18T03:30:08.984950Z","iopub.status.idle":"2025-11-18T03:30:09.539216Z","shell.execute_reply.started":"2025-11-18T03:30:08.984921Z","shell.execute_reply":"2025-11-18T03:30:09.538403Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}