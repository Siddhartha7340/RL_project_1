{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13729513,"sourceType":"datasetVersion","datasetId":8735110}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes peft datasets trl sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:25:10.477534Z","iopub.execute_input":"2025-11-14T10:25:10.477748Z","iopub.status.idle":"2025-11-14T10:26:44.064438Z","shell.execute_reply.started":"2025-11-14T10:25:10.477727Z","shell.execute_reply":"2025-11-14T10:26:44.063251Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom transformers import TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:26:44.850390Z","iopub.execute_input":"2025-11-14T10:26:44.850676Z","iopub.status.idle":"2025-11-14T10:27:16.032013Z","shell.execute_reply.started":"2025-11-14T10:26:44.850642Z","shell.execute_reply":"2025-11-14T10:27:16.031234Z"}},"outputs":[{"name":"stderr","text":"2025-11-14 10:26:57.517343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763116017.701200      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763116017.749783      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# -------------------------\n# Config / paths (edit if needed)\n# -------------------------\nMODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nDATA_PATH = \"/kaggle/input/glaive-coding-assistant/c9bc9129-eba0-4b10-8292-4ae70fc7fa0d.json\"  # replace with your file\nOUTPUT_DIR = \"generator_lora_25k\"\nNUM_SAMPLES = 5000         # <-- reduce to 12k for fast training <1hr\nNUM_EPOCHS = 1\nMAX_LENGTH = 128            # shorter sequences, big speed win\nPER_DEVICE_BATCH = 4\nGRAD_ACCUM = 4  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:27:28.680923Z","iopub.execute_input":"2025-11-14T10:27:28.682002Z","iopub.status.idle":"2025-11-14T10:27:28.686024Z","shell.execute_reply.started":"2025-11-14T10:27:28.681971Z","shell.execute_reply":"2025-11-14T10:27:28.685303Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# Load dataset and format prompts\n# -------------------------\ndef load_qa_dataset(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        raw = json.load(f)\n\n    samples = []\n    for row in raw:\n        q = row.get(\"question\", \"\").strip()\n        a = row.get(\"answer\", \"\").strip()\n        if not q or not a:\n            continue\n\n        # no trailing newline after \"Answer:\" to save tokens\n        prompt = f\"Question: {q}\\nAnswer:\"\n        completion = a\n        samples.append({\"prompt\": prompt, \"completion\": completion})\n\n    return Dataset.from_list(samples)\n\ndataset = load_qa_dataset(DATA_PATH)\nprint(\"Loaded samples:\", len(dataset))\nprint(\"Example sample:\", dataset[0])\n\n# reduce dataset to NUM_SAMPLES (shuffle first)\nif len(dataset) > NUM_SAMPLES:\n    dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))\nprint(\"Using samples:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:27:30.936759Z","iopub.execute_input":"2025-11-14T10:27:30.937041Z","iopub.status.idle":"2025-11-14T10:27:35.605373Z","shell.execute_reply.started":"2025-11-14T10:27:30.937019Z","shell.execute_reply":"2025-11-14T10:27:35.604707Z"}},"outputs":[{"name":"stdout","text":"Loaded samples: 136108\nExample sample: {'prompt': 'Question: How can I output bold text in Bash? I have a Bash script that prints some text to the screen using the `echo \"Some Text\"` command. Is there a way I can format the text to make it bold?\\nAnswer:', 'completion': 'Yes, you can format the output text in Bash to make it bold. Bash allows you to use special escape sequences for text decoration. To make some text bold in bash, you would use the escape sequence `\\\\033[1m`, and to reset the formatting, you would use `\\\\033[0m`. \\n\\nHere\\'s how you can update your `echo` statement to print bold text:\\n\\n```bash\\necho -e \"\\\\033[1mSome Text\\\\033[0m\"\\n```\\n\\nIn this code:\\n\\n- The `-e` option of `echo` allows the interpretation of backslash escapes.\\n- The `\\\\033[1m` sequence sets the text to be bold.\\n- The `Some Text` part is the actual text that will be printed in bold.\\n- The `\\\\033[0m` sequence resets the text formatting to the default, so anything printed afterwards will be in the default format.\\n\\nRemember that these escape sequences may not work in all terminals and circumstances, but they should work in most common situations.'}\nUsing samples: 5000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -------------------------\n# Load tokenizer + model (4-bit)\n# -------------------------\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# fast tokenizer (fallback to use_fast=False if issues)\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nexcept Exception:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Attempting to load 4-bit base model (will try FlashAttention2 then fallback)...\")\n\ndef load_base_model():\n    # try with flash_attention_2 first (if supported by the HF code for the model)\n    try:\n        return AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            attn_implementation=\"flash_attention_2\",\n            trust_remote_code=True,\n        )\n    except Exception:\n        # fallback\n        return AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\n\nbase_model = load_base_model()\n\n# prepare base for k-bit training then apply LoRA\nbase_model = prepare_model_for_kbit_training(base_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:27:35.839643Z","iopub.execute_input":"2025-11-14T10:27:35.839913Z","iopub.status.idle":"2025-11-14T10:29:06.733203Z","shell.execute_reply.started":"2025-11-14T10:27:35.839896Z","shell.execute_reply":"2025-11-14T10:29:06.732625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe8b665f55849998de3599a7b016787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb46361001d4724a0163a22fb1f97b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d506db7159441fb57d213c0038f348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1286f1bd1174fd392d9ade1becb7c79"}},"metadata":{}},{"name":"stdout","text":"Attempting to load 4-bit base model (will try FlashAttention2 then fallback)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d695a0629bb468aa3ce4b522e14d349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc25c1b21614d3192c3aa9748578bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ca80aea0e945638bf5e8b1c66aaaf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3878cc698794f58a9ad7a9e6e0e0b5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1cb59e6c954a00a4ffb5df4832dbc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79a3098b3294fa6bd43c860b6c64826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"487c8ecc659d4383957bdfaa86d5761b"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# -------------------------\n# LoRA config (replace)\n# -------------------------\nlora_cfg = LoraConfig(\n    r=4,                          # smaller rank -> faster\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\"],  # minimal/fast, good for Q/A\n)\n\n\nmodel = get_peft_model(base_model, lora_cfg)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:29:11.289377Z","iopub.execute_input":"2025-11-14T10:29:11.289672Z","iopub.status.idle":"2025-11-14T10:29:11.391124Z","shell.execute_reply.started":"2025-11-14T10:29:11.289648Z","shell.execute_reply":"2025-11-14T10:29:11.390535Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 7,243,436,032 || trainable%: 0.0235\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -------------------------\n# Tokenize function (batched for speed)\n# -------------------------\ndef tokenize_function(examples):\n    prompts = examples[\"prompt\"]\n    completions = examples[\"completion\"]\n    full_texts = [p + c + (tokenizer.eos_token or \"\") for p, c in zip(prompts, completions)]\n\n    # tokenize prompts and full texts (batched) to get prompt lengths and full tokens\n    tokenized_full = tokenizer(\n        full_texts,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=False,\n    )\n    tokenized_prompts = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=False,\n        add_special_tokens=False,\n    )\n\n    labels = []\n    for full_ids, prompt_ids in zip(tokenized_full[\"input_ids\"], tokenized_prompts[\"input_ids\"]):\n        lab = full_ids.copy()\n        prompt_len = len(prompt_ids)\n        # mask prompt tokens (loss only on completion)\n        for i in range(min(prompt_len, len(lab))):\n            lab[i] = -100\n        labels.append(lab)\n\n    return {\n        \"input_ids\": tokenized_full[\"input_ids\"],\n        \"attention_mask\": tokenized_full[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n# batched mapping (faster)\ntokenized_ds = dataset.map(tokenize_function, batched=True, batch_size=2000, remove_columns=dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:29:13.511313Z","iopub.execute_input":"2025-11-14T10:29:13.511600Z","iopub.status.idle":"2025-11-14T10:29:15.663153Z","shell.execute_reply.started":"2025-11-14T10:29:13.511578Z","shell.execute_reply":"2025-11-14T10:29:15.662568Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0004af15ba624dc5bdfd5121640710e2"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# -------------------------\n# Collate function for Trainer\n# -------------------------\ndef collate_fn(batch):\n    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n    labels = [torch.tensor(x[\"labels\"], dtype=torch.long) for x in batch]\n\n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\n    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:29:17.494557Z","iopub.execute_input":"2025-11-14T10:29:17.495063Z","iopub.status.idle":"2025-11-14T10:29:17.500260Z","shell.execute_reply.started":"2025-11-14T10:29:17.495037Z","shell.execute_reply":"2025-11-14T10:29:17.499338Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------\n# TrainingArguments + Trainer (optimized)\n# -------------------------\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=PER_DEVICE_BATCH,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    learning_rate=2e-4,\n    fp16=True,\n    bf16=False,\n    num_train_epochs=NUM_EPOCHS,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    logging_steps=50,\n    remove_unused_columns=False,\n    report_to=\"none\",\n    dataloader_num_workers=4,    # speed up data loading\n)\nimport torch\ntorch.backends.cudnn.benchmark = True\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds,\n    data_collator=collate_fn,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:29:19.226921Z","iopub.execute_input":"2025-11-14T10:29:19.227234Z","iopub.status.idle":"2025-11-14T10:29:19.280875Z","shell.execute_reply.started":"2025-11-14T10:29:19.227172Z","shell.execute_reply":"2025-11-14T10:29:19.280213Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/1001498512.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# -------------------------\n# Train\n# -------------------------\ntrainer.train()\n\n# Save the LoRA adapter (and optionally a small config)\nmodel.save_pretrained(OUTPUT_DIR)\nprint(\"✅ LoRA adapter saved in:\", OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T10:29:21.143931Z","iopub.execute_input":"2025-11-14T10:29:21.144218Z","iopub.status.idle":"2025-11-14T11:42:16.988404Z","shell.execute_reply.started":"2025-11-14T10:29:21.144179Z","shell.execute_reply":"2025-11-14T11:42:16.987597Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 1:12:39, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.570100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.537800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.514500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.503100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.517600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.509800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ LoRA adapter saved in: generator_lora_25k\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# -------------------------\n# Loading for generation (example)\n# -------------------------\n# reload base model (4-bit) and attach adapter for inference\nbase = load_base_model()\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ngen_model = PeftModel.from_pretrained(base, OUTPUT_DIR)\ngen_model.eval()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T11:49:11.449966Z","iopub.execute_input":"2025-11-14T11:49:11.450316Z","iopub.status.idle":"2025-11-14T11:49:11.949041Z","shell.execute_reply.started":"2025-11-14T11:49:11.450293Z","shell.execute_reply":"2025-11-14T11:49:11.947936Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3731563436.py\u001b[0m in \u001b[0;36mload_base_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         return AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4970\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4971\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMistralModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \u001b[0;31m# setting it recursively)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2076\u001b[0;31m         self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n\u001b[0m\u001b[1;32m   2077\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_init_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_check_and_adjust_attn_implementation\u001b[0;34m(self, attn_implementation, is_init_check)\u001b[0m\n\u001b[1;32m   2685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m             applicable_attn_implementation = self.get_correct_attn_implementation(\n\u001b[0m\u001b[1;32m   2687\u001b[0m                 \u001b[0mapplicable_attn_implementation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_init_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mget_correct_attn_implementation\u001b[0;34m(self, requested_attention, is_init_check)\u001b[0m\n\u001b[1;32m   2713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapplicable_attention\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"flash_attention_2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flash_attn_2_can_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_init_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapplicable_attention\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"flash_attention_3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_flash_attn_2_can_dispatch\u001b[0;34m(self, is_init_check)\u001b[0m\n\u001b[1;32m   2421\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flash_attn\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{preface} the package flash_attn seems to be not installed. {install_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3895586833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# reload base model (4-bit) and attach adapter for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3731563436.py\u001b[0m in \u001b[0;36mload_base_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         return AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5027\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5029\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5031\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "],"ename":"ValueError","evalue":"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# Example generation function that returns only the newly generated answer text\ndef generate_answer(question, max_new_tokens=200, **gen_kwargs):\n    prompt = f\"Question: {question}\\nAnswer:\"  # no trailing newline\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n\n    generated = gen_model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=gen_kwargs.get(\"do_sample\", True),\n        temperature=gen_kwargs.get(\"temperature\", 0.8),\n        top_p=gen_kwargs.get(\"top_p\", 0.95),\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n    if decoded.startswith(prompt):\n        return decoded[len(prompt):].strip()\n    if \"Answer:\" in decoded:\n        return decoded.split(\"Answer:\", 1)[1].strip()\n    return decoded.strip()\n\n# Quick test\nq = \"Can someone help me write a MATLAB program to generate a random array of 5 elements?\"\nprint(\"Generated answer:\\n\", generate_answer(q, max_new_tokens=400))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T11:54:51.946433Z","iopub.execute_input":"2025-11-14T11:54:51.946720Z","iopub.status.idle":"2025-11-14T11:55:11.956542Z","shell.execute_reply.started":"2025-11-14T11:54:51.946702Z","shell.execute_reply":"2025-11-14T11:55:11.955705Z"}},"outputs":[{"name":"stdout","text":"Generated answer:\n Certainly! Here's a MATLAB program that generates a random array of 5 elements:\n\n```matlab\n% Generate random array of 5 elements\nn = 5;\n\n% Initialize a random number generator\nrng('default');\n\n% Generate random numbers\na = rand(1,n);\n```\n\nThis program initializes a random number generator with the `rng` function using the default seed. It then uses the `rand` function to generate a random array of 5 elements. The `rand` function returns a single column vector of random numbers between 0 and 1. By passing the desired number of elements (5) to the `rand` function, we create a 1x5 array containing 5 random numbers.\n\nThe generated array, `a`, will contain 5 unique random numbers between 0 and 1. If you need random integers instead, you can multiply the generated numbers by the desired maximum value and round them using the `round` function:\n\n```matlab\n% Generate random integers\nb = round(a * 100);\n```\n\nIn this example, `b` will contain 5 random integers between 0 and 99.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!zip -r output.zip /kaggle/working/generator_lora_25k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T11:51:10.755385Z","iopub.execute_input":"2025-11-14T11:51:10.755682Z","iopub.status.idle":"2025-11-14T11:51:12.491971Z","shell.execute_reply.started":"2025-11-14T11:51:10.755660Z","shell.execute_reply":"2025-11-14T11:51:12.491043Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/generator_lora_25k/ (stored 0%)\n  adding: kaggle/working/generator_lora_25k/README.md (deflated 66%)\n  adding: kaggle/working/generator_lora_25k/adapter_config.json (deflated 55%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/ (stored 0%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/README.md (deflated 66%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/scaler.pt (deflated 60%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/optimizer.pt","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 8%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/special_tokens_map.json (deflated 73%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/tokenizer.json (deflated 85%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/tokenizer.model (deflated 55%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/adapter_config.json (deflated 55%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/tokenizer_config.json (deflated 68%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/training_args.bin (deflated 51%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/scheduler.pt (deflated 56%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/trainer_state.json (deflated 65%)\n  adding: kaggle/working/generator_lora_25k/checkpoint-313/rng_state.pth (deflated 25%)\n  adding: kaggle/working/generator_lora_25k/adapter_model.safetensors (deflated 7%)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from IPython.display import FileLink\nfrom IPython.display import display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T11:53:04.535030Z","iopub.execute_input":"2025-11-14T11:53:04.535333Z","iopub.status.idle":"2025-11-14T11:53:04.538737Z","shell.execute_reply.started":"2025-11-14T11:53:04.535313Z","shell.execute_reply":"2025-11-14T11:53:04.538021Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(FileLink('output.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T11:53:13.645902Z","iopub.execute_input":"2025-11-14T11:53:13.646181Z","iopub.status.idle":"2025-11-14T11:53:13.651813Z","shell.execute_reply.started":"2025-11-14T11:53:13.646160Z","shell.execute_reply":"2025-11-14T11:53:13.651244Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/output.zip","text/html":"<a href='output.zip' target='_blank'>output.zip</a><br>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}